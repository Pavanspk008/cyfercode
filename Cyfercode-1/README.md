ğŸ“„ Final Project README / Summary

You can copyâ€“paste this as-is and tweak wording later if you want.

ğŸ” SIEM Event Classification â€” Signature-Free ML System
ğŸ“Œ Project Overview

This project builds a production-ready machine learning system to classify SIEM security events as malicious or benign.
The focus is on robust detection, interpretability, and operational reliability, rather than chasing complex models or inflated metrics.

The system is designed to generalize beyond known attack signatures and remain effective against unseen or zero-day behaviors.

ğŸ¯ Problem Statement

Traditional signature-based security systems perform well on known attacks but struggle with:

unseen attack variants

noisy or missing signatures

frequent rule maintenance

The goal of this project was to:

detect attacks using behavioral and contextual signals

avoid over-reliance on predefined attack names

produce decisions that SOC teams can understand, tune, and trust

ğŸ§  Approach & Modeling Strategy
1ï¸âƒ£ Baseline V1 â€” Naive Model

Logistic Regression with raw alert metadata

Achieved perfect metrics

Feature importance revealed heavy reliance on alert signatures
âŒ Identified as shortcut learning

2ï¸âƒ£ Baseline V2-A â€” Signature-Reduced

Replaced raw attack names with abstracted behavioral flags

Improved generalization

Served as a feature engineering validation step

3ï¸âƒ£ Baseline V2-B â€” Signature-Free (Final Model)

Removed all attack signatures

Relied purely on:

behavioral intensity (e.g., failed attempts)

contextual signals (IP type, protocol, severity)

Achieved near-perfect recall with realistic false positives

Demonstrated robust, behavior-driven learning

âœ… Selected for production deployment

ğŸ¤– Final Model

Algorithm: Logistic Regression

Design Choice: Signature-free, interpretable, stable

Why not complex models?

Interpretability for SOC teams

Easier monitoring and debugging

Configurable thresholds without retraining

ğŸ§ª Evaluation Strategy

Confusion matrix and class-wise metrics

Threshold analysis (security-focused, not accuracy-only)

Stress testing via signature removal

Feature importance via model coefficients

ğŸš€ Production Inference Design
Inference Characteristics

Stateless, single-event inference

Same pipeline used for training and inference

Outputs probability score + final decision

Decision Logic

Model outputs attack_probability

SOC-controlled threshold determines final alert

Sensitivity can be adjusted without retraining

ğŸ“Š Monitoring & Drift Strategy

After deployment, the system monitors:

Prediction distribution drift

Key feature drift (severity, failed attempts, IP type)

Alert volume & SOC workload

Performance metrics when labels become available

This ensures early detection of silent failures.

ğŸ”„ Retraining Strategy

Retraining triggered by:

feature drift

prediction drift

operational feedback

performance degradation

New models are validated offline

Rollback plan is always maintained

ğŸ“¦ Project Structure (Logical)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ baseline_v1.ipynb
â”‚   â”œâ”€â”€ baseline_v2.ipynb
â”‚   â””â”€â”€ production_inference_v2b.ipynb
â”œâ”€â”€ models/
â”‚   â””â”€â”€ siem_lr_v2b_pipeline.joblib
â””â”€â”€ README.md

ğŸ¤ Key Takeaway

This project demonstrates how to build a reliable, interpretable, and production-ready ML system for cybersecurity â€” focusing on behavior, robustness, and operational control rather than model complexity.

ğŸ§¾ One-Line Summary (Resume-Ready)

Built a signature-free, interpretable ML system for SIEM event classification with production-grade inference, monitoring, and retraining design.

























ğŸ” SIEM Event Classification â€” Signature-Free ML System

Production-grade machine learning system for behavioral cybersecurity detection

ğŸš€ Overview

This project implements a signature-free, interpretable machine learning system to classify SIEM security events as malicious or benign.

Unlike traditional rule-based systems, the model focuses on behavioral and contextual signals, making it more robust to unseen or zero-day attacks while remaining fully explainable for SOC teams.

ğŸ¯ Key Objectives

Detect attacks without relying on predefined signatures

Maintain high recall with controlled alert noise

Provide interpretable decisions for SOC analysts

Design the system with production readiness in mind

ğŸ§  Modeling Strategy
Baseline Evolution
Baseline	Description	Key Insight
V1	Naive Logistic Regression	Detected shortcut learning via signatures
V2-A	Signature-Reduced	Preserved performance with abstracted signals
V2-B âœ…	Signature-Free	Behavioral learning with realistic noise

ğŸ‘‰ Baseline V2-B was selected for production deployment

ğŸ¤– Final Model (Deployed)

Algorithm: Logistic Regression

Design: Signature-free, behavior-driven

Why Logistic Regression?

Transparent coefficients

SOC-friendly explanations

Stable probability outputs

Easy threshold control

ğŸ“Š Core Features Used
Behavioral Signals

failed_attempts

session_duration

bytes_sent

dst_port

Contextual Signals

src_ip_type

protocol

event_type

alert_severity

is_cloud_asset

âŒ No attack names
âŒ No rule-based labels
âŒ No signature leakage

âš™ï¸ Production Inference Flow

Receive a single SIEM event

Validate & convert to tabular format

Generate attack probability

Apply configurable SOC threshold

Return structured decision output

{
  "attack_probability": 0.99996,
  "threshold": 0.7,
  "prediction": "ATTACK",
  "model": "logistic_regression",
  "baseline": "V2-B"
}

ğŸ” Evaluation Highlights

Near-perfect recall under signature-free stress testing

Smooth threshold behavior (no probability collapse)

Realistic false positives (expected in security systems)

Performance degradation was intentional and desirable to validate robustness.

ğŸ“ˆ Monitoring & Drift Strategy

After deployment, the system monitors:

ğŸ“‰ Prediction distribution drift

ğŸ“Š Key feature drift (severity, IP type, failed attempts)

ğŸš¨ Alert volume & SOC workload

ğŸ¯ Delayed performance metrics

This ensures early detection of silent failures.

ğŸ”„ Retraining Strategy

Triggered by:

Feature drift

Prediction drift

SOC feedback

Performance degradation

Offline validation before redeploy

Rollback-ready model versioning

ğŸ—‚ï¸ Project Structure
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ baseline_v1.ipynb
â”‚   â”œâ”€â”€ baseline_v2.ipynb
â”‚   â””â”€â”€ production_inference_v2b.ipynb
â”œâ”€â”€ models/
â”‚   â””â”€â”€ siem_lr_v2b_pipeline.joblib

ğŸ§¾ Key Takeaway

This project demonstrates how to build a production-ready, interpretable ML system that detects behavioral risk rather than memorizing attack signatures.

ğŸ“Œ Resume-Ready One-Liner

Built a signature-free, interpretable ML system for SIEM event classification with production-grade inference, monitoring, and retraining design.